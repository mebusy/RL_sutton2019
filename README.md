
# S.Sutton  Reinforcement Learning: An Introduction

http://incompleteideas.net/book/

http://incompleteideas.net/book/solutions-2nd.html

Recommend :

- covering Chapter 1 for a brief overview.
- Chapter 2 through Section 2.4, Chapter 3, and then selecting sections from the remaining chapters according to time and interests.
- Chapter 6 is the most important for the subject and for the rest of the book.
- artificial intelligence or planning :  Chapter 8
- machine learning or neural networks :  Chapter 9, 10


marked as `*` :  more difficult and not essential to the rest of the book.  These can be omitted on 1st reading. 


# Part I: Tabular Solution Methods

[Part I](book/Part%20I%20Tabular%20Solution%20Methods/Part%20I.pdf)

The simplest RL forms: the approximate value functions to be represented as *arrays*, or *tables* , and can ofen find *exact solution*, while general RL can only find approximate solutions. 

1. bandit problem
    - only a single state
2. MDP ,  3 fundamental mothods to solve MDP:
    - dynamic programming
        - well developed mathematically
        - but require a complete and accurate model of the environment
    - Monte Carlo methods
        - donâ€™t require a model and are conceptually simple
        - but are not well suited for step-by-step incremental computation.
    - temporal-difference learning 
        - require no model and are fully incremental
        - but are more complex to analyze.
3. combining those 3 methods to solve MDP

## Chaper 2. Multi-armed Bandits

[Chapter 2](book/Part%20I%20Tabular%20Solution%20Methods/02.%20Multi-armed%20Bandits.pdf)

choose of e 

nonstationary problem

step size 
1.
2.
3.


### 2.6 Optimistic Initial Values



[soft-max](https://github.com/mebusy/notes/blob/master/dev_notes/softmax.md)








